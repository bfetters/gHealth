{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gHealth by Brandon Fetters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final project created during my Data Engineering course at [GalvanizeU](http://www.galvanizeu.com/). We were required to create an application architecture which would be able to support big data requirements.\n",
    "\n",
    "I chose to develop an application for tracking individual health information with [myfitnesspal](https://www.myfitnesspal.com/) as my inspiration.\n",
    "\n",
    "The project was divided into the following steps:\n",
    "1. [Define Avro Schema](#Define-Avro-Schema)\n",
    "2. [Generate Data](#Generate-Data)\n",
    "3. [Data Serialization](#Data-Serialization)\n",
    "4. [Vertical Partitioning](#Vertical-Partitioning)\n",
    "5. [Batch Views](#Batch-Views) ([Create](#Create-Batch-Views),[Query](#Query-Batch-Views))\n",
    "6. [Speed Layer](#Speed-Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Avro Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='gHealth_GraphSchema.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import avro.io\n",
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gHealth_schema.avsc\n"
     ]
    }
   ],
   "source": [
    "%%writefile gHealth_schema.avsc\n",
    "[\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Pedigree\",\n",
    "        \"fields\": [{\"name\": \"true_as_of_secs\", \"type\": \"int\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"UserID1\",\n",
    "        \"fields\": [{\"name\": \"cookie\", \"type\": \"string\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"UserID2\",\n",
    "        \"fields\": [{\"name\": \"user_id\", \"type\": \"string\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"GroupID\",\n",
    "        \"fields\": [{\"name\": \"group_id\", \"type\": \"string\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"GoalID\",\n",
    "        \"fields\": [{\"name\": \"goal_id\", \"type\": \"string\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"MealID\",\n",
    "        \"fields\": [{\"name\": \"meal_id\", \"type\": \"string\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"ActivityID\",\n",
    "        \"fields\": [{\"name\": \"activity_id\", \"type\": \"string\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"UserProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": [\"UserID1\",\"UserID2\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"UserPropertyValue1\",\n",
    "                        \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"UserPropertyValue3\",\n",
    "                        \"fields\": [{\"name\": \"dob\", \"type\": \"int\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"UserPropertyValue4\",\n",
    "                        \"fields\": [\n",
    "                            {\n",
    "                                \"name\": \"gender\", \n",
    "                                \"type\": {\n",
    "                                    \"type\": \"enum\",\n",
    "                                    \"name\": \"GenderType\",\n",
    "                                    \"symbols\": [\"MALE\", \"FEMALE\"]\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"UserPropertyValue5\",\n",
    "                        \"fields\": [\n",
    "                            {\n",
    "                                \"name\": \"location\", \n",
    "                                \"type\": {\n",
    "                                    \"type\": \"record\",\n",
    "                                    \"name\": \"Location\",\n",
    "                                    \"fields\": [\n",
    "                                        {\"name\": \"zipcode\", \"type\": [\"int\", \"null\"]},\n",
    "                                        {\"name\": \"city\", \"type\": [\"string\", \"null\"]},\n",
    "                                        {\"name\": \"state\", \"type\": [\"string\", \"null\"]},\n",
    "                                        {\"name\": \"country\", \"type\": [ \"string\",\"null\"]}\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"GroupProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"group_id\",\n",
    "                \"type\": \"GroupID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"GroupPropertyValue1\",\n",
    "                        \"fields\": [{\"name\": \"group_name\", \"type\": \"string\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"GoalProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"goal_id\",\n",
    "                \"type\": \"GoalID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"GoalPropertyValue1\",\n",
    "                        \"fields\": [{\"name\": \"goal_name\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"GoalPropertyValue2\",\n",
    "                        \"fields\": [{\"name\": \"goal_desc\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"GoalPropertyValue3\",\n",
    "                        \"fields\": [{\"name\": \"goal_value\", \"type\": \"int\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"GoalPropertyValue4\",\n",
    "                        \"fields\": [{\"name\": \"goal_unit\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"GoalPropertyValue5\",\n",
    "                        \"fields\": [{\"name\": \"goal_status\", \"type\": \"string\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }      \n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"MealProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"meal_id\",\n",
    "                \"type\": \"MealID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"MealPropertyValue1\",\n",
    "                        \"fields\": [{\"name\": \"meal_type\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"MealPropertyValue2\",\n",
    "                        \"fields\": [{\"name\": \"meal_name\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"MealPropertyValue3\",\n",
    "                        \"fields\": [{\"name\": \"meal_desc\", \"type\": \"string\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },     \n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"ActivityProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"activity_id\",\n",
    "                \"type\": \"ActivityID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"ActivityPropertyValue1\",\n",
    "                        \"fields\": [{\"name\": \"activity_type\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"ActivityPropertyValue2\",\n",
    "                        \"fields\": [{\"name\": \"activity_length\", \"type\": \"int\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"ActivityPropertyValue3\",\n",
    "                        \"fields\": [{\"name\": \"activity_unit\", \"type\": \"string\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },    \n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"EquivEdge\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"user_id1\", \"type\": [\"UserID1\", \"UserID2\"]},\n",
    "            {\"name\": \"user_id2\", \"type\": [\"UserID1\", \"UserID2\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"GroupEdge\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"group_id\", \"type\": \"GroupID\"},\n",
    "            {\"name\": \"user_id\", \"type\": [\"UserID1\", \"UserID2\"]},\n",
    "            {\"name\": \"role\", \"type\": {\"type\": \"enum\",\n",
    "                                      \"name\": \"RoleType\",\n",
    "                                      \"symbols\": [\"OWNER\", \"MEMBER\"]}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"GoalEdge\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"user_id\", \"type\": [\"UserID1\", \"UserID2\"]},\n",
    "            {\"name\": \"goal_id\", \"type\": \"GoalID\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"MealEdge\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"user_id\", \"type\": [\"UserID1\", \"UserID2\"]},\n",
    "            {\"name\": \"meal_id\", \"type\": \"MealID\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"ActivityEdge\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"user_id\", \"type\": [\"UserID1\", \"UserID2\"]},\n",
    "            {\"name\": \"activity_id\", \"type\": \"ActivityID\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"gHealth.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Data\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"pedigree\",\n",
    "                \"type\": \"Pedigree\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"dataunit\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit1\",\n",
    "                        \"fields\": [{\"name\": \"user_property\", \"type\": \"UserProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit2\",\n",
    "                        \"fields\": [{\"name\": \"equiv\", \"type\": \"EquivEdge\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit3\",\n",
    "                        \"fields\": [{\"name\": \"group_property\", \"type\": \"GroupProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit4\",\n",
    "                        \"fields\": [{\"name\": \"group_edge\", \"type\": \"GroupEdge\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit5\",\n",
    "                        \"fields\": [{\"name\": \"goal_property\", \"type\": \"GoalProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit6\",\n",
    "                        \"fields\": [{\"name\": \"goal_edge\", \"type\": \"GoalEdge\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit7\",\n",
    "                        \"fields\": [{\"name\": \"meal_property\", \"type\": \"MealProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit8\",\n",
    "                        \"fields\": [{\"name\": \"meal_edge\", \"type\": \"MealEdge\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit9\",\n",
    "                        \"fields\": [{\"name\": \"activity_property\", \"type\": \"ActivityProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit10\",\n",
    "                        \"fields\": [{\"name\": \"activity_edge\", \"type\": \"ActivityEdge\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "[top](#gHealth-by-Brandon-Fetters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gen_gHealth_data.py\n"
     ]
    }
   ],
   "source": [
    "%%file gen_gHealth_data.py\n",
    "from datetime import datetime\n",
    "import calendar,random,time,uuid\n",
    "\n",
    "# Various libraries for data generation\n",
    "import names\n",
    "from barnum import gen_data\n",
    "    \n",
    "### Create data generator ###\n",
    "def generate_data(n_users=1,n_meals=1,n_activities=1,gen_goals=True,gen_groups=True):\n",
    "    \n",
    "    # List to store all created data which is returned as output\n",
    "    data = []\n",
    "    \n",
    "    # Create generator for unique id's to be used for nodes\n",
    "    get_uuid = gen_uuids()\n",
    "    user_ids = [get_uuid.next() for i in xrange(n_users)]\n",
    "    users_sample = perform_ratio_sampling(user_ids,n_users,.5)\n",
    "    \n",
    "    ### GENERATE NODES\n",
    "    # USERS\n",
    "    locations = generate_locations(n_users)\n",
    "    for user_id in user_ids:\n",
    "        data += generate_user(user_id,locations)\n",
    "\n",
    "    # MEALS\n",
    "    for i in xrange(n_meals):\n",
    "        data += generate_meal(get_uuid,users_sample)\n",
    "        \n",
    "    # ACTIVITIES\n",
    "    for i in xrange(n_activities):\n",
    "        data += generate_activity(get_uuid,users_sample)\n",
    "\n",
    "    # GOALS\n",
    "    if gen_goals:\n",
    "        data += generate_goals(get_uuid,user_ids,n_users)\n",
    "\n",
    "    # GROUPS\n",
    "    if gen_groups:\n",
    "        group_owners = perform_ratio_sampling(user_ids,n_users,.15)\n",
    "        for group_owner in group_owners:\n",
    "            data += generate_group(get_uuid,user_ids,group_owner,n_users)\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_user(user_id,locations):\n",
    "    '''    \n",
    "    [{\"name\": \"user_property\", \"type\": \"UserProperty\"}]\n",
    "    user_id, name, dob, height, weight, gender, location\n",
    "    '''\n",
    "    \n",
    "    # Reference lists for randomly generating values\n",
    "    genders = ['MALE','FEMALE']\n",
    "    \n",
    "    # Create variables for any random generated values which need to be used multiple times\n",
    "    # (e.g., names and cookies)\n",
    "    gender = random.choice(genders)\n",
    "    name = names.get_full_name(gender=gender.lower())\n",
    "    dob = random.randrange(-315619200,1104451200) # (1/1/1960 - 12/31/2004) in seconds since epoch\n",
    "    zipcode,city,state,country = random.choice(locations)\n",
    "    height = random.randint(60,78)\n",
    "    weight = random.randint(90,350)\n",
    "    \n",
    "    # Generate random timestamp from range.\n",
    "    # TODO: Need to account for cases when \"child\" has timestamp earlier than \"parent\".\n",
    "    ts = create_timestamp(90,14)\n",
    "    cookie = ''.join(random.sample(str(ts),len(str(ts))))\n",
    "    \n",
    "    # Generate single user\n",
    "    user = [\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"user_property\": {\"user_id\": {\"user_id\": user_id},\n",
    "                                           \"property\": {\"name\": name}}}},\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"user_property\": {\"user_id\": {\"user_id\": user_id},\n",
    "                                           \"property\": {\"dob\": int(dob)}}}},\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"user_property\": {\"user_id\": {\"user_id\": user_id},\n",
    "                                           \"property\": {\"gender\": gender}}}},\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"user_property\": {\"user_id\": {\"user_id\": user_id},\n",
    "                                           \"property\": {\"location\": {\"zipcode\" : zipcode,\n",
    "                                                                     \"city\" : city,\n",
    "                                                                     \"state\": state,\n",
    "                                                                     \"country\": country}}}}}\n",
    "           ]\n",
    "    \n",
    "    ## Also generate equiv edge for every user\n",
    "    # [{\"name\": \"equiv\", \"type\": \"EquivEdge\"}]\n",
    "    # user_id1,user_id2\n",
    "    equiv = [{\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "              \"dataunit\": {\"equiv\": {\"user_id1\": {\"cookie\": cookie},\n",
    "                                     \"user_id2\": {\"user_id\": user_id}}}}]\n",
    "    \n",
    "    return user + equiv\n",
    "\n",
    "def generate_meal(get_uuid,users_sample):\n",
    "    '''\n",
    "    [{\"name\": \"meal_property\", \"type\": \"MealProperty\"}]\n",
    "    meal_id, meal_type, meal_name, meal_desc\n",
    "    '''\n",
    "    \n",
    "    # Reference lists for randomly generating values\n",
    "    meal_types = ['Breakfast','Lunch','Dinner','Snack']\n",
    "    meal_desc = ['Popcorn','Doritos','Seafood Paella','Som Tam','Chicken Rice','Poutine',\n",
    "                 'Fish Tacos','French Toast','Chicken Parmigiana', 'Pulled Pork Sandwich',\n",
    "                 'Chili Crab','Pancakes with Syrup','Fish N Chips','Ankimo','Sushi',\n",
    "                 'Steak','Chicken Fajitas','Lasagna','Brownie and Vanilla Ice Cream',\n",
    "                 'Almond Croissant','Twix','Arepas','Nam Tok Moo','Chicken Kebab','Lobster',\n",
    "                 'Sour Patch Kids','Chocolate Covered Donut','Shepherds Pie','Rendang',\n",
    "                 'Chicken Muamba','Tom Yum Goong','Penang Assam Laksa','Cheesburger','Pizza',\n",
    "                 'Spinach Salad','Protein Shake','Peking Duck','Larb Gai','Palak Paneer',\n",
    "                 'Cheerios','Mac N Cheese','Hot Dog','NY Cheesecake','PB & J','Cashews',\n",
    "                 'Chile Rellenos','Nachos','Chicken Wings','Meatloaf','Egg Custard']\n",
    "    \n",
    "    # Create variables for any random generated values which need to be used multiple times\n",
    "    # Generate random timestamp from range.\n",
    "    meal_id = get_uuid.next()\n",
    "    # TODO: Need to account for cases when \"child\" (edge node) has timestamp\n",
    "    # earlier than \"parent\" (user node).\n",
    "    ts = create_timestamp(90,14)\n",
    "    \n",
    "    # Generate single meal\n",
    "    meal = [\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"meal_property\": {\"meal_id\": {\"meal_id\": meal_id},\n",
    "                                           \"property\": {\"meal_type\": random.choice(meal_types)}}}},\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"meal_property\": {\"meal_id\": {\"meal_id\": meal_id},\n",
    "                                           \"property\": {\"meal_name\": random.choice(meal_desc)}}}},\n",
    "           {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "            \"dataunit\": {\"meal_property\": {\"meal_id\": {\"meal_id\": meal_id},\n",
    "                                           \"property\": {\"meal_desc\": gen_data.create_sentence()}}}}\n",
    "           ]\n",
    "    \n",
    "    ## Also generate meal_edge for at least 1 user\n",
    "    # [{\"name\": \"meal_edge\", \"type\": \"MealEdge\"}]\n",
    "    # user_id, meal_id\n",
    "    meal_edge = [{\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                  \"dataunit\": {\"meal_edge\": {\"meal_id\": {\"meal_id\": meal_id},\n",
    "                                             \"user_id\": {\"user_id\": random.choice(users_sample)}}}}]\n",
    "        \n",
    "    return meal + meal_edge\n",
    "\n",
    "def generate_activity(get_uuid,users_sample):\n",
    "    '''\n",
    "    [{\"name\": \"activity_property\", \"type\": \"ActivityProperty\"}]\n",
    "    activity_id, activity_type, activity_length, activity_unit\n",
    "    '''\n",
    "    \n",
    "    # Reference lists for randomly generating values\n",
    "    activity_types = ['Running','Cycling','Mountain Biking','Walking','Hiking','Downhill Skiing',\n",
    "                      'Cross Country Skiing','Snowboarding','Skating','Swimming','Rowing',\n",
    "                      'Crossfit','Weight Training','Other']\n",
    "    activity_units = ['Miles','Laps','Hours','Minutes']\n",
    "    \n",
    "    # Create variables for any random generated values which need to be used multiple times\n",
    "    # (e.g., names and cookies)\n",
    "    activity_id = get_uuid.next()\n",
    "    activity_length = random.randint(1,50)\n",
    "    # Generate random timestamp from range.\n",
    "    # TODO: Need to account for cases when \"child\" has timestamp earlier than \"parent\".\n",
    "    ts = create_timestamp(90,14)\n",
    "    \n",
    "    # Generate single activity\n",
    "    activity = [\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"activity_property\": {\"activity_id\": {\"activity_id\": activity_id},\n",
    "                                                   \"property\": {\"activity_type\": random.choice(activity_types)}}}},\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"activity_property\": {\"activity_id\": {\"activity_id\": activity_id},\n",
    "                                                   \"property\": {\"activity_length\": activity_length}}}},\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"activity_property\": {\"activity_id\": {\"activity_id\": activity_id},\n",
    "                                                   \"property\": {\"activity_unit\": random.choice(activity_units)}}}}\n",
    "               ]\n",
    "    \n",
    "    ## Also generate activity_edge for at least 1 user\n",
    "    # [{\"name\": \"activity_edge\", \"type\": \"ActivityEdge\"}]\n",
    "    # user_id, activity_id\n",
    "    activity_edge = [{\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                      \"dataunit\": {\"activity_edge\": {\"activity_id\": {\"activity_id\": activity_id},\n",
    "                                                     \"user_id\": {\"user_id\": random.choice(users_sample)}}}}]\n",
    "    \n",
    "    return activity + activity_edge\n",
    "\n",
    "def generate_goals(get_uuid,user_ids,n_users):\n",
    "    '''    \n",
    "    [{\"name\": \"goal_property\", \"type\": \"GoalProperty\"}]\n",
    "    goal_id, goal_name, goal_desc, goal_value, goal_unit, goal_status\n",
    "    [{\"name\": \"goal_edge\", \"type\": \"GoalEdge\"}]\n",
    "    user_id, goal_id\n",
    "    '''\n",
    "    \n",
    "    # Reference lists for randomly generating values\n",
    "    # Create goals for a random sample of 10% of users.\n",
    "    goal_users = perform_ratio_sampling(user_ids,n_users,.1)\n",
    "    goals_text = [('Lose Weight','Lose ' + str(random.randint(5,30)) + ' pounds.'),\n",
    "                  ('Exercise','Exercise ' + str(random.randint(3,7)) + ' times this week.'),\n",
    "                  ('Eat out less.','Only eat out ' + str(random.randint(1,3)) + ' times per week.'),\n",
    "                  ('Get more sleep', 'Get ' + str(random.randint(8,12)) + ' hours of sleep per night'),\n",
    "                  ('Cut out sugar','Cut sugar intake by ' + str(random.choice(['50%','60%','75%']))),\n",
    "                  ('Workout!','Workout for at least 30 minutes every day this week.')]\n",
    "    goal_status = ['Not Started','Underway','Continuous','Completed']\n",
    "    \n",
    "    goals = []\n",
    "    for user_id in goal_users:\n",
    "        # Create variables for any random generated values which need to be used multiple times\n",
    "        goal_id = get_uuid.next()\n",
    "        # Generate random timestamp from range.\n",
    "        # TODO: Need to account for cases when \"child\" has timestamp earlier than \"parent\".\n",
    "        ts = create_timestamp(90,14)\n",
    "        goal_name,goal_desc = random.choice(goals_text)\n",
    "        \n",
    "        goal = [\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"goal_property\": {\"goal_id\": {\"goal_id\": goal_id},\n",
    "                                               \"property\": {\"goal_name\": goal_name}}}},\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"goal_property\": {\"goal_id\": {\"goal_id\": goal_id},\n",
    "                                               \"property\": {\"goal_desc\": goal_desc}}}},\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"goal_property\": {\"goal_id\": {\"goal_id\": goal_id},\n",
    "                                               \"property\": {\"goal_status\": random.choice(goal_status)}}}},\n",
    "               # Go ahead and create goal_edge inline with the goal itself.\n",
    "               {\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                \"dataunit\": {\"goal_edge\": {\"goal_id\": {\"goal_id\": goal_id},\n",
    "                                           \"user_id\": {\"user_id\": user_id}}}}\n",
    "               ]\n",
    "        goals += goal\n",
    "    \n",
    "    return goals\n",
    "\n",
    "def generate_group(get_uuid,user_ids,group_owner,n_users):\n",
    "    '''\n",
    "    [{\"name\": \"group_property\", \"type\": \"GroupProperty\"}]\n",
    "    group_id, name\n",
    "    [{\"name\": \"group_edge\", \"type\": \"GroupEdge\"}]\n",
    "    group_id, user_id, role\n",
    "    '''\n",
    "    \n",
    "    # Reference lists for randomly generating values\n",
    "    group_names = ['Family','Friends','Coworkers','Elite','Workout Fun','Siblings','Freedom']\n",
    "    \n",
    "    # Create variables for any random generated values which need to be used multiple times\n",
    "    group_id = get_uuid.next()\n",
    "    group_name = random.choice(group_names) + '_' + str(group_owner)\n",
    "    ts = create_timestamp(90,14)\n",
    "    \n",
    "    group = [{\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "             \"dataunit\": {\"group_property\": {\"group_id\": {\"group_id\": group_id},\n",
    "                                             \"property\": {\"group_name\": group_name}}}}]\n",
    "    \n",
    "    group_edges = [{\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                    \"dataunit\": {\"group_edge\": {\"group_id\": {\"group_id\": group_id},\n",
    "                                                \"user_id\": {\"user_id\": group_owner},\n",
    "                                                \"role\": 'OWNER'}}}]\n",
    "    for group_member in random.sample(user_ids,random.randint(3,min(n_users,15))):\n",
    "        group_edge = [{\"pedigree\": {\"true_as_of_secs\": ts},\n",
    "                       \"dataunit\": {\"group_edge\": {\"group_id\": {\"group_id\": group_id},\n",
    "                                                   \"user_id\": {\"user_id\": group_member},\n",
    "                                                   \"role\": 'MEMBER'}}}]\n",
    "        group_edges += group_edge\n",
    "    \n",
    "    return group + group_edges    \n",
    "\n",
    "### HELPER METHODS ###\n",
    "def gen_uuids():\n",
    "    while True:\n",
    "        yield str(uuid.uuid4())\n",
    "        \n",
    "def create_timestamp(back,forward):\n",
    "    '''\n",
    "    Using the current time generate a timestamp within the range of 90 days in the past\n",
    "    and 14 days in the future.\n",
    "    INPUTS: \n",
    "        - back(int): how many days back should the range start\n",
    "        - forward(int): how many days forward should the range end\n",
    "    OUTPUTS:\n",
    "        - ts(int): randomly generated timestamp from range\n",
    "    '''\n",
    "    start = int(time.time() - (back * 86400))\n",
    "    end = int(time.time() + (forward * 86400))\n",
    "    return random.randrange(start,end)\n",
    "\n",
    "def generate_locations(n_users,location_ratio=.1):\n",
    "    '''\n",
    "    Generate a list of locations stored as a tuple of (city,state,country). The number of locations\n",
    "    generated will depend on the desired ratio of users to locations. The default location_ratio\n",
    "    is low to ensure a percentage of users will share locations.\n",
    "    INPUTS:\n",
    "        - n_users(int): Number of users being generated\n",
    "        - location_ratio(float): The desired ratio for n_users by n_locations.\n",
    "    OUTPUTS:\n",
    "        - locs (list): List of generated location tuples in the form (city,state,country)\n",
    "    '''\n",
    "    locs = []\n",
    "    # Based on the desired ratio of locations to users\n",
    "    if int(n_users * location_ratio) < 1:\n",
    "        n_locs = 1\n",
    "    else:\n",
    "        n_locs = int(n_users * location_ratio)\n",
    "    \n",
    "    for n in xrange(n_locs):\n",
    "        zipcode,city,state = gen_data.create_city_state_zip()\n",
    "        locs.append((int(zipcode),city,state,'US'))\n",
    "    \n",
    "    return locs\n",
    "\n",
    "def perform_ratio_sampling(population,count,percent):\n",
    "    '''\n",
    "    Control number of non-primary nodes created by randomly sampling a\n",
    "    population based on a ratio. For example, not all users would be expected to\n",
    "    create activities, so randomly sample the population of users and only create\n",
    "    activities for a percentage of the users.\n",
    "    INPUTS:\n",
    "        - population(list): data to be sampled\n",
    "        - count(int): total count to be used for sampling. this is required because\n",
    "                      you cannot just use len() for generators\n",
    "        - percent(float): the percent to use for our sampling ratio\n",
    "    '''\n",
    "    # The number of samples must be at least 1\n",
    "    if count < 2:\n",
    "        n_samples = 1\n",
    "    else:\n",
    "        n_samples = int(count * percent)\n",
    "    return random.sample(population,n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pretty import pprint\n",
    "\n",
    "VALIDATE = False\n",
    "\n",
    "# Generate testing data and output for validation\n",
    "n_users = 5000\n",
    "n_meals = n_users * 4\n",
    "n_activities = n_users * 3\n",
    "test_data = generate_data(n_users=n_users,n_meals=n_meals,n_activities=n_activities)\n",
    "\n",
    "schema = avro.schema.parse(open(\"gHealth_schema.avsc\").read())\n",
    "def test_good_data_health(datum, schema=schema):\n",
    "    return avro.io.validate(schema, datum)\n",
    "\n",
    "# print data for quick sanity check on values\n",
    "if VALIDATE:\n",
    "#     pprint(test_data)\n",
    "    print map(test_good_data_health, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Serialization\n",
    "[top](#gHealth-by-Brandon-Fetters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete old file\n",
    "!rm gHealth.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 175306 records processed.\n",
      "10000 of 175306 records processed.\n",
      "20000 of 175306 records processed.\n",
      "30000 of 175306 records processed.\n",
      "40000 of 175306 records processed.\n",
      "50000 of 175306 records processed.\n",
      "60000 of 175306 records processed.\n",
      "70000 of 175306 records processed.\n",
      "80000 of 175306 records processed.\n",
      "90000 of 175306 records processed.\n",
      "100000 of 175306 records processed.\n",
      "110000 of 175306 records processed.\n",
      "120000 of 175306 records processed.\n",
      "130000 of 175306 records processed.\n",
      "140000 of 175306 records processed.\n",
      "150000 of 175306 records processed.\n",
      "160000 of 175306 records processed.\n",
      "170000 of 175306 records processed.\n",
      "175306 of 175306 records processed.\n"
     ]
    }
   ],
   "source": [
    "# Importing required packages For file Read & Write \n",
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter  \n",
    "from avro.io import DatumReader, DatumWriter\n",
    " \n",
    "# Schema parsing from a schema file\n",
    "schema = avro.schema.parse(open(\"gHealth_schema.avsc\").read())\n",
    " \n",
    "# Creation of DataFileWriter instance with above schema\n",
    "writer = DataFileWriter(open(\"gHealth.avro\", \"w\"), DatumWriter(), schema)\n",
    " \n",
    "# Loop through generated data and append one at a time to the file.\n",
    "total = len(test_data)-1 # Subtract one because i will start at 0\n",
    "for i,dataunit in enumerate(test_data):\n",
    "    if i % 10000 == 0 or i == total:\n",
    "        print \"{} of {} records processed.\".format(i,total)\n",
    "    writer.append(dataunit)\n",
    "    \n",
    "# Close the data file\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'dataunit': {u'user_property': {u'property': {u'name': u'Janet Crumpton'},\n",
      "   u'user_id': {u'user_id': u'96708369-055f-422e-abed-91bca7d16a12'}}},\n",
      " u'pedigree': {u'true_as_of_secs': 1441214997}}\n",
      "None\n",
      "{u'dataunit': {u'user_property': {u'property': {u'dob': 471118225},\n",
      "   u'user_id': {u'user_id': u'96708369-055f-422e-abed-91bca7d16a12'}}},\n",
      " u'pedigree': {u'true_as_of_secs': 1441214997}}\n",
      "None\n",
      "{u'dataunit': {u'user_property': {u'property': {u'gender': u'FEMALE'},\n",
      "   u'user_id': {u'user_id': u'96708369-055f-422e-abed-91bca7d16a12'}}},\n",
      " u'pedigree': {u'true_as_of_secs': 1441214997}}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read the above created avro data file. Opening file in read mode for quick sanity check. \n",
    "reader = DataFileReader(open(\"gHealth.avro\", \"r\"), DatumReader())\n",
    "for i,dataunit in enumerate(reader):\n",
    "    if i < 3:\n",
    "        print pprint(dataunit)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Close the avro data file after completion of reading it.\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to AWS S3 bucket\n",
    "from boto.s3.connection import S3Connection\n",
    "conn = S3Connection(AWS_ACCESS_KEY,AWS_SECRET_ACCESS_KEY)\n",
    "bucket = conn.get_bucket('gu6007')\n",
    "\n",
    "# Upload generated data avro file to S3 bucket\n",
    "filename = \"gHealth.avro\"\n",
    "print 'Uploading %s to Amazon S3 bucket %s' % (filename, bucket)\n",
    "\n",
    "# Output to track status\n",
    "import sys\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "from boto.s3.key import Key\n",
    "k = Key(bucket)\n",
    "k.key = filename\n",
    "k.set_contents_from_filename(filename,cb=percent_cb, num_cb=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Partitioning\n",
    "[top](#gHealth-by-Brandon-Fetters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fastavro\n",
    "\n",
    "from cStringIO import StringIO\n",
    "import pyspark as ps\n",
    "\n",
    "sc = ps.SparkContext()\n",
    "sqlContext = ps.HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_data(datum):\n",
    "    datatype = datum['dataunit'].keys()[0]\n",
    "    if datatype.endswith('property'):\n",
    "        return '/'.join((datatype, datum['dataunit'][datatype]['property'].keys()[0])), datum\n",
    "    else:\n",
    "        return datatype, datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[34] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = sc.parallelize(bucket.get_all_keys(prefix='gHealth'))\n",
    "avro_data = keys.map(lambda key: StringIO(key.get_contents_as_string()))\n",
    "json_data = avro_data.flatMap(fastavro.reader)\n",
    "partitioned_json = json_data.map(partition_data)\n",
    "partitioned_json.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove old partitioned data\n",
    "!rm -R /Users/bfetters/projects/github/gHealth/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition_names = partitioned_json.map(lambda t: t[0]).distinct().collect()\n",
    "\n",
    "for p in partition_names:\n",
    "    path = \"../gHealth/master/{}\".format(p)\n",
    "    if os.path.exists(path):\n",
    "        print \"{} exists\".format(path)\n",
    "    else:\n",
    "        partitioned_json.filter(lambda t: t[0] == p).values().saveAsPickleFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../gHealth/\r\n",
      "└── master\r\n",
      "    ├── activity_edge\r\n",
      "    │   ├── _SUCCESS\r\n",
      "    │   ├── part-00000\r\n",
      "    │   ├── part-00001\r\n",
      "    │   ├── part-00002\r\n",
      "    │   └── part-00003\r\n",
      "    ├── activity_property\r\n",
      "    │   ├── activity_length\r\n",
      "    │   │   ├── _SUCCESS\r\n",
      "    │   │   ├── part-00000\r\n",
      "    │   │   ├── part-00001\r\n",
      "    │   │   ├── part-00002\r\n",
      "    │   │   └── part-00003\r\n",
      "    │   ├── activity_type\r\n",
      "    │   │   ├── _SUCCESS\r\n",
      "    │   │   ├── part-00000\r\n",
      "    │   │   ├── part-00001\r\n",
      "    │   │   ├── part-00002\r\n",
      "    │   │   └── part-00003\r\n",
      "    │   └── activity_unit\r\n",
      "    │       ├── _SUCCESS\r\n",
      "    │       ├── part-00000\r\n",
      "    │       ├── part-00001\r\n",
      "    │       ├── part-00002\r\n",
      "    │       └── part-00003\r\n",
      "    ├── equiv\r\n",
      "    │   ├── _SUCCESS\r\n",
      "    │   ├── part-00000\r\n",
      "    │   ├── part-00001\r\n",
      "    │   ├── part-00002\r\n",
      "    │   └── part-00003\r\n",
      "    ├── goal_edge\r\n",
      "    │   ├── _SUCCESS\r\n",
      "    │   ├── part-00000\r\n",
      "    │   ├── part-00001\r\n",
      "    │   ├── part-00002\r\n",
      "    │   └── part-00003\r\n",
      "    ├── goal_property\r\n",
      "    │   ├── goal_desc\r\n",
      "    │   │   ├── _SUCCESS\r\n",
      "    │   │   ├── part-00000\r\n",
      "    │   │   ├── part-00001\r\n",
      "    │   │   ├── part-00002\r\n",
      "    │   │   └── part-00003\r\n",
      "    │   ├── goal_name\r\n",
      "    │   │   ├── _SUCCESS\r\n",
      "    │   │   ├── part-00000\r\n",
      "    │   │   ├── part-00001\r\n",
      "    │   │   ├── part-00002\r\n",
      "    │   │   └── part-00003\r\n",
      "    │   └── goal_status\r\n",
      "    │       ├── _SUCCESS\r\n",
      "    │       ├── part-00000\r\n",
      "    │       ├── part-00001\r\n",
      "    │       ├── part-00002\r\n",
      "    │       └── part-00003\r\n",
      "    ├── group_edge\r\n",
      "    │   ├── _SUCCESS\r\n",
      "    │   ├── part-00000\r\n",
      "    │   ├── part-00001\r\n",
      "    │   ├── part-00002\r\n",
      "    │   └── part-00003\r\n",
      "    ├── group_property\r\n",
      "    │   └── group_name\r\n",
      "    │       ├── _SUCCESS\r\n",
      "    │       ├── part-00000\r\n",
      "    │       ├── part-00001\r\n",
      "    │       ├── part-00002\r\n",
      "    │       └── part-00003\r\n",
      "    ├── meal_edge\r\n",
      "    │   ├── _SUCCESS\r\n",
      "    │   ├── part-00000\r\n",
      "    │   ├── part-00001\r\n",
      "    │   ├── part-00002\r\n",
      "    │   └── part-00003\r\n",
      "    ├── meal_property\r\n",
      "    │   ├── meal_desc\r\n",
      "    │   │   ├── _SUCCESS\r\n",
      "    │   │   ├── part-00000\r\n",
      "    │   │   ├── part-00001\r\n",
      "    │   │   ├── part-00002\r\n",
      "    │   │   └── part-00003\r\n",
      "    │   ├── meal_name\r\n",
      "    │   │   ├── _SUCCESS\r\n",
      "    │   │   ├── part-00000\r\n",
      "    │   │   ├── part-00001\r\n",
      "    │   │   ├── part-00002\r\n",
      "    │   │   └── part-00003\r\n",
      "    │   └── meal_type\r\n",
      "    │       ├── _SUCCESS\r\n",
      "    │       ├── part-00000\r\n",
      "    │       ├── part-00001\r\n",
      "    │       ├── part-00002\r\n",
      "    │       └── part-00003\r\n",
      "    └── user_property\r\n",
      "        ├── dob\r\n",
      "        │   ├── _SUCCESS\r\n",
      "        │   ├── part-00000\r\n",
      "        │   ├── part-00001\r\n",
      "        │   ├── part-00002\r\n",
      "        │   └── part-00003\r\n",
      "        ├── gender\r\n",
      "        │   ├── _SUCCESS\r\n",
      "        │   ├── part-00000\r\n",
      "        │   ├── part-00001\r\n",
      "        │   ├── part-00002\r\n",
      "        │   └── part-00003\r\n",
      "        ├── location\r\n",
      "        │   ├── _SUCCESS\r\n",
      "        │   ├── part-00000\r\n",
      "        │   ├── part-00001\r\n",
      "        │   ├── part-00002\r\n",
      "        │   └── part-00003\r\n",
      "        └── name\r\n",
      "            ├── _SUCCESS\r\n",
      "            ├── part-00000\r\n",
      "            ├── part-00001\r\n",
      "            ├── part-00002\r\n",
      "            └── part-00003\r\n",
      "\r\n",
      "25 directories, 95 files\r\n"
     ]
    }
   ],
   "source": [
    "# Quick validation that vertical partition was created as expected\n",
    "!tree ../gHealth/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Views\n",
    "[top](#gHealth-by-Brandon-Fetters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Batch Views\n",
    "- [User Features Aggregation](#User-Features-Aggregation)\n",
    "- [Count of Activities by User by Type](#Count-of-Activities-by-User-by-Type)\n",
    "- [Count of Meals by User by Type](#Count-of-Meals-by-User-by-Type)\n",
    "- [Count of Activities by Location by Type](#Count-of-Activities-by-Location-by-Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pretty import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For now, go ahead and create an initial rdd for each node and edge to make our lives easier\n",
    "# later when we create batch views.\n",
    "path = '../gHealth/master/'\n",
    "\n",
    "# USER\n",
    "user_dob_pkl = sc.pickleFile(path + 'user_property/dob')\n",
    "user_loc_pkl = sc.pickleFile(path + 'user_property/location/')\n",
    "user_name_pkl = sc.pickleFile(path + 'user_property/name')\n",
    "user_gender_pkl = sc.pickleFile(path + 'user_property/gender')\n",
    "equiv_edge_pkl = sc.pickleFile(path + 'equiv')\n",
    "\n",
    "# ACTIVITY\n",
    "act_type_pkl = sc.pickleFile(path + 'activity_property/activity_type')\n",
    "act_length_pkl = sc.pickleFile(path + 'activity_property/activity_length')\n",
    "act_unit_pkl = sc.pickleFile(path + 'activity_property/activity_unit')\n",
    "act_edge_pkl = sc.pickleFile(path + 'activity_edge')\n",
    "\n",
    "# MEAL\n",
    "meal_name_pkl = sc.pickleFile(path + 'meal_property/meal_name')\n",
    "meal_type_pkl = sc.pickleFile(path + 'meal_property/meal_type')\n",
    "meal_desc_pkl = sc.pickleFile(path + 'meal_property/meal_desc')\n",
    "meal_edge_pkl = sc.pickleFile(path + 'meal_edge')\n",
    "\n",
    "# GOAL\n",
    "goal_name_pkl = sc.pickleFile(path + 'goal_property/goal_name')\n",
    "goal_name_pkl = sc.pickleFile(path + 'goal_property/goal_name')\n",
    "goal_desc_pkl = sc.pickleFile(path + 'goal_property/goal_desc')\n",
    "goal_status_pkl = sc.pickleFile(path + 'goal_property/goal_status')\n",
    "goal_edge_pkl = sc.pickleFile(path + 'goal_edge')\n",
    "    \n",
    "# GROUP\n",
    "group_name_pkl = sc.pickleFile(path + 'group_property/group_name')\n",
    "group_edge_pkl = sc.pickleFile(path + 'group_edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User Features Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime,date\n",
    "\n",
    "# TODO: Need to come back to this and make it more generic, but closing the loop\n",
    "# on the end to end process first.\n",
    "def user_kv_to_dict(joined_data):\n",
    "    '''\n",
    "    Given the cumulative joined (key,value) tuples from multiple joins we need to parse the data out\n",
    "    with the intention of returning a dictionary which will provide the schema for our spark dataframe.\n",
    "    \n",
    "    NOTE: The method as currently constructed only works for the User Features Aggregated joined data.\n",
    "    INPUTS:\n",
    "        - joined_data(tuple): key-value tuple with nested value tuples from a series of joins\n",
    "    OUTPUTS:\n",
    "        - result_dict(dict): parsed and reformated data from user joined key-value tuple to dictionary\n",
    "    '''\n",
    "    (user_id,((((((((name,gender),age),(city,state)),last_ts),act_count),meal_count),goal_count),group_count)) = joined_data\n",
    "    result_dict = {'user_id'    : user_id,\n",
    "                   'name'       : name,\n",
    "                   'gender'     : gender,\n",
    "                   'age'        : age,\n",
    "                   'city'       : city,\n",
    "                   'state'      : state,\n",
    "                   'status'     : is_active(last_ts),\n",
    "                   'act_count'  : act_count,\n",
    "                   'meal_count' : meal_count,\n",
    "                   'goal_count' : goal_count,\n",
    "                   'group_count': group_count}\n",
    "    return result_dict\n",
    "\n",
    "def calculate_age(dob):\n",
    "    '''\n",
    "    Given a date of birth in seconds since epoch, convert to age.\n",
    "    INPUTS:\n",
    "        - dob(int): date of birth stored in seconds since epoch\n",
    "    OUTPUTS:\n",
    "        - age(int): current age based on date of birth\n",
    "    '''\n",
    "    today = date.today()\n",
    "    born = datetime.fromtimestamp(dob)\n",
    "    age = today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "    return age\n",
    "\n",
    "def is_active(ts):\n",
    "    '''\n",
    "    Calculates whether a given event has occurred in the last 30-60 days. If within the\n",
    "    last 30 days a user is considered to be active, if within the last 60 days a user\n",
    "    is considered to be inactive, and if longer than 60 days a user is considered to have\n",
    "    stopped using the app.\n",
    "    INPUTS:\n",
    "        - ts(int): event timestamp stored as seconds since epoch\n",
    "    OUTPUTS:\n",
    "        - status(str): status of user given last event: [active, inactive, or churn]\n",
    "    '''\n",
    "    now = int(time.time())\n",
    "    sixty_days = now - (60 * 86400)\n",
    "    thirty_days = now - (30 * 86400)\n",
    "    if ts < sixty_days:\n",
    "        return 'churn'\n",
    "    elif ts < thirty_days:\n",
    "        return 'inactive'\n",
    "    else:\n",
    "        return 'active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "--- LAST USER ACTION ---\n",
    "(used later during user feature batch view aggregation)\n",
    "\n",
    "1. Create (k,v) rdd for (user_id,pedigree) of last activity\n",
    "2. Create (k,v) rdd for (user_id,pedigree) of last meal\n",
    "3. Join last activity and last meal to find latest event for each user.\n",
    "'''\n",
    "\n",
    "# 1. Create (k,v) rdd for (user_id,pedigree) of last activity\n",
    "last_user_act = act_edge_pkl.map(lambda datum: (datum['dataunit']['activity_edge']['user_id']['user_id'],\n",
    "                                                datum['pedigree']['true_as_of_secs']))\\\n",
    "                            .reduceByKey(lambda ts1,ts2: ts1)\n",
    "# last_user_act.take(5)\n",
    "\n",
    "# 2. Create (k,v) rdd for (user_id,pedigree) of last meal\n",
    "last_user_meal = meal_edge_pkl.map(lambda datum: (datum['dataunit']['meal_edge']['user_id']['user_id'],\n",
    "                                                  datum['pedigree']['true_as_of_secs']))\\\n",
    "                              .reduceByKey(lambda ts1,ts2: ts1)\n",
    "# last_user_meal.take(5)\n",
    "\n",
    "# 3. Join last activity and last meal to find latest event for each user.\n",
    "last_user_event = last_user_act.join(last_user_meal).map(lambda (user_id,(ts1,ts2)): (user_id,max(ts1,ts2)))\n",
    "# last_user_event.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- act_count: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- goal_count: long (nullable = true)\n",
      " |-- group_count: long (nullable = true)\n",
      " |-- meal_count: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+-------------+------+---+-------------+-----+---------+----------+----------+-----------+--------+\n",
      "|         name|gender|age|         city|state|act_count|meal_count|goal_count|group_count|  status|\n",
      "+-------------+------+---+-------------+-----+---------+----------+----------+-----------+--------+\n",
      "|  Bettie Holt|FEMALE| 22|       Starke|   FL|        5|         7|      null|          2|  active|\n",
      "|   Jill Flynn|FEMALE| 21|     Thompson|   OH|        8|        10|      null|          1|   churn|\n",
      "|Robert Foster|  MALE| 21|    Lafayette|   MN|        7|        12|      null|          1|  active|\n",
      "|  Carol Davis|FEMALE| 36|  Springfield|   IL|        5|         7|      null|          1|  active|\n",
      "|Amanda Stroud|FEMALE| 36|Sweet Springs|   MO|        8|         9|      null|          1|  active|\n",
      "|Donna Garrett|FEMALE| 36|     Rockland|   MA|        4|         5|      null|          2|inactive|\n",
      "|  Tasha Kempf|FEMALE| 33|       Hiller|   PA|        4|         6|         1|          2|  active|\n",
      "|   Logan Khan|  MALE| 42|      Lenorah|   TX|        2|         6|      null|       null|inactive|\n",
      "|   Laura Fitz|FEMALE| 46|     Bluebell|   UT|        7|        12|      null|          1|  active|\n",
      "|  Carrie Geer|FEMALE| 33|     Savannah|   GA|        9|         6|      null|          1|   churn|\n",
      "+-------------+------+---+-------------+-----+---------+----------+----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "--- USER FEATURES AGGREGATION ---\n",
    "1. Create (user,name) pair rdd\n",
    "2. Create (user,gender) pair rdd\n",
    "3. Create (user,age) pair rdd\n",
    "4. Create (user,location) pair rdd\n",
    "5. Create (user,activity_count) pair rdd\n",
    "6. Create (user,meal_count) pair rdd\n",
    "7. Create (k,v) rdd for (user_id,goal_count)\n",
    "8. Create (k,v) rdd for (user_id,group_count)\n",
    "9. Join 1-8 into single rdd for \"user feature\" rdd\n",
    "'''\n",
    "\n",
    "# 1. Create (k,v) rdd for (user_id,user_name)\n",
    "user_name_pair = user_name_pkl.map(lambda datum: (datum['dataunit']['user_property']['user_id']['user_id'],\n",
    "                                                  datum['dataunit']['user_property']['property']['name']))\n",
    "# pprint(user_name_pair.take(5))\n",
    "\n",
    "# 2. Create (k,v) rdd for (user_id,gender)\n",
    "user_gender_pair = user_gender_pkl.map(lambda datum: (datum['dataunit']['user_property']['user_id']['user_id'],\n",
    "                                                      datum['dataunit']['user_property']['property']['gender']))\n",
    "# pprint(user_gender_pair.take(5))\n",
    "\n",
    "# 3. Create (k,v) rdd for (user_id,(dob,age))\n",
    "user_dob_pair = user_dob_pkl.map(lambda datum: (datum['dataunit']['user_property']['user_id']['user_id'],\n",
    "                                                datum['dataunit']['user_property']['property']['dob']))\\\n",
    "                            .map(lambda (user_id,dob): (user_id,calculate_age(dob)))\n",
    "# pprint(user_dob_pair.take(5))\n",
    "\n",
    "# 4. Create (k,v) rdd for (user_id,(city,state))\n",
    "user_loc_pair = user_loc_pkl.map(lambda datum: (datum['dataunit']['user_property']['user_id']['user_id'],\n",
    "                                                (datum['dataunit']['user_property']['property']['location']['city'],\n",
    "                                                 datum['dataunit']['user_property']['property']['location']['state'])))\n",
    "# pprint(user_loc_pair.take(5))\n",
    "\n",
    "# 5. Create (k,v) rdd for (user_id,act_count)\n",
    "user_act_count = act_edge_pkl.map(lambda datum: (datum['dataunit']['activity_edge']['user_id']['user_id'],1))\\\n",
    "                             .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\n",
    "# pprint(user_act_count.take(5))\n",
    "\n",
    "# 6. Create (k,v) rdd for (user_id,meal_count)\n",
    "user_meal_count = meal_edge_pkl.map(lambda datum: (datum['dataunit']['meal_edge']['user_id']['user_id'],1))\\\n",
    "                               .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\n",
    "# pprint(user_meal_count.take(5))\n",
    "\n",
    "# 7. Create (k,v) rdd for (user_id,goal_count)\n",
    "user_goal_count = goal_edge_pkl.map(lambda datum: (datum['dataunit']['goal_edge']['user_id']['user_id'],1))\\\n",
    "                               .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\n",
    "# pprint(user_goal_count.take(5))\n",
    "\n",
    "# 8. Create (k,v) rdd for (user_id,group_count)\n",
    "user_group_count = group_edge_pkl.map(lambda datum: (datum['dataunit']['group_edge']['user_id']['user_id'],1))\\\n",
    "                               .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\n",
    "# pprint(user_group_count.take(5))\n",
    "\n",
    "# 9. Join all the user features into single aggregated rdd\n",
    "users_pair = user_name_pair.join(user_gender_pair)\\\n",
    "                           .join(user_dob_pair)\\\n",
    "                           .join(user_loc_pair)\\\n",
    "                           .join(last_user_event)\\\n",
    "                           .leftOuterJoin(user_act_count)\\\n",
    "                           .leftOuterJoin(user_meal_count)\\\n",
    "                           .leftOuterJoin(user_goal_count)\\\n",
    "                           .leftOuterJoin(user_group_count)\\\n",
    "                           .map(lambda joined_data: user_kv_to_dict(joined_data))\n",
    "# pprint(users_pair.take(5))\n",
    "\n",
    "# Read in rdd to create DataFrame, register table, and print schema for validation\n",
    "users_df = sqlContext.createDataFrame(users_pair.collect())\n",
    "users_df.registerTempTable('users')\n",
    "\n",
    "# Print schema and head of dataframe for validation\n",
    "users_df.printSchema()\n",
    "user_features = sqlContext.sql('select name,gender,age,city,state,act_count, \\\n",
    "                                       meal_count,goal_count,group_count,status \\\n",
    "                                from users order by user_id')\n",
    "user_features.show(10)\n",
    "\n",
    "# Write batch view to parquet file\n",
    "user_features.write.parquet('../gHealth/batch/user_features/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count of Activities by User by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- act_type: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+------------+--------+-----+\n",
      "|        name|act_type|count|\n",
      "+------------+--------+-----+\n",
      "|David Wilson|Swimming|    2|\n",
      "|David Wilson|  Rowing|    1|\n",
      "|David Wilson| Skating|    1|\n",
      "|David Wilson|   Other|    1|\n",
      "|David Wilson| Walking|    1|\n",
      "|David Wilson|  Hiking|    1|\n",
      "|David Wilson| Cycling|    1|\n",
      "|   Adam Koch| Walking|    1|\n",
      "|   Adam Koch| Cycling|    1|\n",
      "|   Adam Koch| Running|    2|\n",
      "+------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Create (k,v) rdd for (activity_id,user_id)\n",
    "2. Create (k,v) rdd for (activity_id,activity_type)\n",
    "3. Create (k,v) rdd for (user_id,(activity_type,count))\n",
    "4. Join with user_name_pair to get name and output as dict\n",
    "'''\n",
    "\n",
    "# 1. Create (k,v) rdd for (activity_id,user_id)\n",
    "act_user_id_pair = act_edge_pkl.map(lambda datum: (datum['dataunit']['activity_edge']['activity_id']['activity_id'],\n",
    "                                                   datum['dataunit']['activity_edge']['user_id']['user_id']))\n",
    "# pprint(act_user_id_pair.take(5))\n",
    "\n",
    "# 2. Create (k,v) rdd for (activity_id,activity_type)\n",
    "act_id_type_pair = act_type_pkl.map(lambda datum: (datum['dataunit']['activity_property']['activity_id']['activity_id'],\n",
    "                                                   datum['dataunit']['activity_property']['property']['activity_type']))\n",
    "# pprint(act_id_type_pair.take(5))\n",
    "\n",
    "# 3. Create (k,v) rdd for (user_id,(activity_type,count))\n",
    "user_act_type_pair = act_user_id_pair.join(act_id_type_pair)\\\n",
    "                                     .map(lambda (act_id,(user_id,act_type)): ((user_id,act_type),1))\\\n",
    "                                     .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\\\n",
    "                                     .map(lambda ((user_id,act_type),count): (user_id,(act_type,count)))\n",
    "# pprint(user_act_type_pair.take(5))\n",
    "\n",
    "# 4. Join with user_name_pair to get name and output as dict\n",
    "act_type_by_user = user_name_pair.join(user_act_type_pair)\\\n",
    "                                 .map(lambda (user_id,(name,(act_type,count))): {'user_id': user_id,\n",
    "                                                                                 'name': name,\n",
    "                                                                                 'act_type': act_type,\n",
    "                                                                                 'count': count})\n",
    "# pprint(act_by_user.take(5))\n",
    "\n",
    "# # Read in rdd to create DataFrame, register table, and print schema for validation\n",
    "act_type_by_user_df = sqlContext.createDataFrame(act_type_by_user.collect())\n",
    "act_type_by_user_df.registerTempTable('act_by_user')\n",
    "\n",
    "# Print schema and head of dataframe for validation\n",
    "act_type_by_user_df.printSchema()\n",
    "activities_user = sqlContext.sql('select name,act_type,count from act_by_user')\n",
    "activities_user.show(10)\n",
    "\n",
    "# Write batch view to parquet file\n",
    "activities_user.write.parquet('../gHealth/batch/activities_user/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count of Meals by User by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- count: long (nullable = true)\n",
      " |-- meal_type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n",
      "+-------------+---------+-----+\n",
      "|         name|meal_type|count|\n",
      "+-------------+---------+-----+\n",
      "| David Wilson|    Snack|    3|\n",
      "| David Wilson|    Lunch|    1|\n",
      "| David Wilson|Breakfast|    2|\n",
      "|    Adam Koch|    Lunch|    2|\n",
      "|    Adam Koch|   Dinner|    2|\n",
      "|    Adam Koch|    Snack|    3|\n",
      "|Mary Armstead|Breakfast|    3|\n",
      "|Mary Armstead|    Snack|    1|\n",
      "|Mary Armstead|    Lunch|    1|\n",
      "|Mary Armstead|   Dinner|    3|\n",
      "+-------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Create (k,v) rdd for (meal_id,user_id)\n",
    "2. Create (k,v) rdd for (meal_id,meal_type)\n",
    "3. Create (k,v) rdd for (user_id,(meal_type,count))\n",
    "4. Join with user_name_pair to get name and output as dict\n",
    "'''\n",
    "\n",
    "# 1. Create (k,v) rdd for (user_id,meal_id)\n",
    "meal_user_id_pair = meal_edge_pkl.map(lambda datum: (datum['dataunit']['meal_edge']['meal_id']['meal_id'],\n",
    "                                                     datum['dataunit']['meal_edge']['user_id']['user_id']))\n",
    "# pprint(meal_user_id_pair.take(5))\n",
    "\n",
    "# 2. Create (k,v) rdd for (meal_id,meal_type)\n",
    "meal_id_type_pair = meal_type_pkl.map(lambda datum: (datum['dataunit']['meal_property']['meal_id']['meal_id'],\n",
    "                                                     datum['dataunit']['meal_property']['property']['meal_type']))\n",
    "# pprint(meal_id_type_pair.take(5))\n",
    "\n",
    "# 3. Create (k,v) rdd for (user_id,(meal_type,count))\n",
    "user_meal_type_pair = meal_user_id_pair.join(meal_id_type_pair)\\\n",
    "                                       .map(lambda (meal_id,(user_id,meal_type)): ((user_id,meal_type),1))\\\n",
    "                                       .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\\\n",
    "                                       .map(lambda ((user_id,meal_type),count): (user_id,(meal_type,count)))\n",
    "# pprint(user_meal_type_pair.take(5))\n",
    "\n",
    "# 4. Join with user_name_pair to get name and output as dict\n",
    "meal_type_by_user = user_name_pair.join(user_meal_type_pair)\\\n",
    "                                   .map(lambda (user_id,(name,(meal_type,count))): {'user_id': user_id,\n",
    "                                                                                    'name': name,\n",
    "                                                                                    'meal_type': meal_type,\n",
    "                                                                                    'count': count})\n",
    "# pprint(meal_type_by_user.take(5))\n",
    "\n",
    "# Read in rdd to create DataFrame, register table, and print schema for validation\n",
    "meal_type_by_user_df = sqlContext.createDataFrame(meal_type_by_user.collect())\n",
    "meal_type_by_user_df.registerTempTable('meal_by_user')\n",
    "\n",
    "# Print schema and head of dataframe for validation\n",
    "meal_type_by_user_df.printSchema()\n",
    "meals_user = sqlContext.sql('select name,meal_type,count from meal_by_user')\n",
    "meals_user.show(10)\n",
    "\n",
    "# Write batch view to parquet file\n",
    "meals_user.write.parquet('../gHealth/batch/meals_user/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count of Activities by Location by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- act_type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+-----+-----+---------------+-----+\n",
      "|state| city|       act_type|count|\n",
      "+-----+-----+---------------+-----+\n",
      "|   SC|Aiken|        Walking|    1|\n",
      "|   SC|Aiken|          Other|    1|\n",
      "|   SC|Aiken|         Rowing|    1|\n",
      "|   SC|Aiken|        Cycling|    1|\n",
      "|   OH|Akron|        Skating|    1|\n",
      "|   OH|Akron|       Crossfit|    1|\n",
      "|   TX|Alice|        Running|    3|\n",
      "|   TX|Alice|Weight Training|    3|\n",
      "|   TX|Alice|        Cycling|    3|\n",
      "|   TX|Alice|         Rowing|    3|\n",
      "+-----+-----+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Create (user,location) pair rdd\n",
    "2. Join with earlier created user_act_type_pair\n",
    "'''\n",
    "\n",
    "# 1. Create (k,v) rdd for (user_id,(city,state))\n",
    "user_loc_pair = user_loc_pkl.map(lambda datum: (datum['dataunit']['user_property']['user_id']['user_id'],\n",
    "                                                (datum['dataunit']['user_property']['property']['location']['city'],\n",
    "                                                 datum['dataunit']['user_property']['property']['location']['state'])))\n",
    "# pprint(user_loc_pair.take(5))\n",
    "\n",
    "# 2. Create (k,v) rdd for (location,(activity_type,count))\n",
    "act_type_per_loc = user_loc_pair.join(user_act_type_pair)\\\n",
    "                                .map(lambda (user_id,(location,(act_type, count))): ((location,act_type),count))\\\n",
    "                                .reduceByKey(lambda cnt1,cnt2: cnt1 + cnt2)\\\n",
    "                                .map(lambda ((location,act_type),count): (location,(act_type,count)))\\\n",
    "                                .sortBy(lambda (location,act_count): location,ascending=True)\\\n",
    "                                .map(lambda ((city,state),(act_type,count)): {'city':city,\n",
    "                                                                              'state':state,\n",
    "                                                                              'act_type':act_type,\n",
    "                                                                              'count':count})\n",
    "                        \n",
    "# pprint(act_type_per_loc.take(5))\n",
    "\n",
    "# Read in rdd to create DataFrame, register table, and print schema for validation\n",
    "act_type_per_loc_df = sqlContext.createDataFrame(act_type_per_loc)\n",
    "act_type_per_loc_df.registerTempTable('act_by_loc')\n",
    "\n",
    "# Print schema and head of dataframe for validation\n",
    "act_type_per_loc_df.printSchema()\n",
    "activities_loc = sqlContext.sql('select state,city,act_type,count from act_by_loc')\n",
    "activities_loc.show(10)\n",
    "\n",
    "# Write batch view to parquet file\n",
    "activities_loc.write.parquet('../gHealth/batch/activities_loc/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Batch Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of users by state\n",
      "+-----+-----+\n",
      "|state|total|\n",
      "+-----+-----+\n",
      "|   PA|  166|\n",
      "|   TX|  153|\n",
      "|   CA|  141|\n",
      "|   NY|  118|\n",
      "|   OH|  103|\n",
      "|   IL|   96|\n",
      "|   WV|   80|\n",
      "|   NJ|   76|\n",
      "|   MN|   69|\n",
      "|   MA|   69|\n",
      "|   AL|   68|\n",
      "|   MO|   68|\n",
      "|   NE|   66|\n",
      "|   FL|   65|\n",
      "|   WI|   64|\n",
      "|   LA|   63|\n",
      "|   NC|   62|\n",
      "|   VA|   58|\n",
      "|   MI|   57|\n",
      "|   GA|   56|\n",
      "+-----+-----+\n",
      "\n",
      "Average age of users by state\n",
      "+-----+-------+\n",
      "|state|avg_age|\n",
      "+-----+-------+\n",
      "|   DC|     46|\n",
      "|   SD|     39|\n",
      "|   AR|     37|\n",
      "|   DE|     37|\n",
      "|   NM|     36|\n",
      "|   IA|     35|\n",
      "|   MS|     35|\n",
      "|   CO|     35|\n",
      "|   SC|     34|\n",
      "|   IL|     34|\n",
      "|   WV|     34|\n",
      "|   VT|     34|\n",
      "|   OK|     34|\n",
      "|   VA|     33|\n",
      "|   NH|     33|\n",
      "|   ND|     33|\n",
      "|   NJ|     33|\n",
      "|   AK|     33|\n",
      "|   WY|     33|\n",
      "|   AZ|     33|\n",
      "+-----+-------+\n",
      "\n",
      "Customer Churn Metrics\n",
      "+--------+-----+\n",
      "|  status|count|\n",
      "+--------+-----+\n",
      "|  active| 1631|\n",
      "|   churn|  221|\n",
      "|inactive|  642|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' User Features '''\n",
    "# Load user features batch view, register, and query\n",
    "# users = sqlContext.read.parquet('../gHealth/batch/user_features')\n",
    "# users.registerTempTable(\"users\");\n",
    "\n",
    "print \"Count of users by state\"\n",
    "sqlContext.sql(\"\"\"select state,count(*) as total\n",
    "                  from users\n",
    "                  group by state\n",
    "                  order by total desc\"\"\").show()\n",
    "\n",
    "print \"Average age of users by state\"\n",
    "sqlContext.sql(\"\"\"select state,floor(avg(age)) as avg_age\n",
    "                  from users\n",
    "                  group by state\n",
    "                  order by avg_age desc\"\"\").show()\n",
    "\n",
    "print \"Customer Churn Metrics\"\n",
    "sqlContext.sql(\"\"\"select status,count(*) as count\n",
    "                  from users\n",
    "                  group by status\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Activities by User > 3\n",
      "+-----------------+--------------------+-----+\n",
      "|             name|            act_type|count|\n",
      "+-----------------+--------------------+-----+\n",
      "|    Thomas Barris|     Weight Training|    4|\n",
      "|    Karen Mcateer|        Snowboarding|    5|\n",
      "|       Lula Davis|Cross Country Skiing|    4|\n",
      "|   Donald Herring|     Mountain Biking|    4|\n",
      "|      Betty Drake|             Skating|    4|\n",
      "|     Joseph Neese|            Crossfit|    4|\n",
      "|      Nancy Magee|             Running|    4|\n",
      "|  Carolyn Tindell|            Crossfit|    4|\n",
      "|     Kelly Monroy|        Snowboarding|    4|\n",
      "|   Thelma Leatham|             Running|    4|\n",
      "|  Charles Carroll|        Snowboarding|    4|\n",
      "|       Penny Cahn|     Downhill Skiing|    4|\n",
      "|   Kevin Fountain|             Skating|    4|\n",
      "|    Kim Alexander|             Running|    4|\n",
      "|Thomasina Simmons|            Crossfit|    5|\n",
      "|    Linda Calumag|            Swimming|    5|\n",
      "|    Randy Compton|             Running|    4|\n",
      "|      Janet Inman|     Downhill Skiing|    4|\n",
      "|   Kristin Harper|              Hiking|    4|\n",
      "| Kristina Schlick|              Rowing|    4|\n",
      "+-----------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Activities by User '''\n",
    "# Load activities by user batch view, register, and query\n",
    "activities_user = sqlContext.read.parquet('../gHealth/batch/activities_user')\n",
    "activities_user.registerTempTable(\"activities_user\");\n",
    "\n",
    "print \"Count of Activities by User > 3\"\n",
    "sqlContext.sql(\"\"\"select name,act_type,count\n",
    "                  from activities_user\n",
    "                  where count > 3\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular activity (or activities) in each state\n",
      "+-----+--------------------+---+\n",
      "|state|            act_type|cnt|\n",
      "+-----+--------------------+---+\n",
      "|   AK|             Cycling|  4|\n",
      "|   AL|            Crossfit| 12|\n",
      "|   AL|        Snowboarding| 12|\n",
      "|   AL|             Walking| 12|\n",
      "|   AL|     Downhill Skiing| 12|\n",
      "|   AR|               Other|  6|\n",
      "|   AR|     Downhill Skiing|  6|\n",
      "|   AR|             Walking|  6|\n",
      "|   AR|Cross Country Skiing|  6|\n",
      "|   AZ|              Rowing|  5|\n",
      "|   AZ|            Crossfit|  5|\n",
      "|   AZ|     Weight Training|  5|\n",
      "|   AZ|     Mountain Biking|  5|\n",
      "|   AZ|             Cycling|  5|\n",
      "|   CA|     Weight Training| 24|\n",
      "|   CO|               Other|  6|\n",
      "|   CO|              Hiking|  6|\n",
      "|   CO|     Downhill Skiing|  6|\n",
      "|   CO|             Cycling|  6|\n",
      "|   CO|             Running|  6|\n",
      "+-----+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Activities by Location '''\n",
    "# Load activities by location batch view, register, and query\n",
    "activities_loc = sqlContext.read.parquet('../gHealth/batch/activities_loc')\n",
    "activities_loc.registerTempTable(\"activities_loc\");\n",
    "\n",
    "print \"Most popular activity (or activities) in each state\"\n",
    "act_cnt_by_state = sqlContext.sql(\"\"\"select state,act_type,count(*) as cnt\n",
    "                                     from activities_loc\n",
    "                                     group by state,act_type\n",
    "                                     order by state,act_type,cnt desc\"\"\")\n",
    "act_cnt_by_state.registerTempTable(\"act_cnt_by_state\");\n",
    "\n",
    "sqlContext.sql(\"\"\"select acbs2.state,acbs2.act_type,acbs2.cnt\n",
    "                  from (select state,max(cnt) as max_cnt\n",
    "                        from act_cnt_by_state\n",
    "                        group by state\n",
    "                        order by state) as acbs1\n",
    "                  inner join act_cnt_by_state as acbs2\n",
    "                  on acbs2.state = acbs1.state and acbs2.cnt = acbs1.max_cnt\n",
    "                  order by acbs2.state\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Layer\n",
    "[top](#gHealth-by-Brandon-Fetters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ./bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "# ./bin/kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io, random, threading, logging, time, json\n",
    "\n",
    "import avro.io\n",
    "import avro.schema\n",
    "\n",
    "from kafka.client   import KafkaClient\n",
    "from kafka.consumer import KafkaConsumer\n",
    "from kafka.producer import SimpleProducer\n",
    "\n",
    "from gen_gHealth_data import generate_data\n",
    "\n",
    "KAFKA_TOPIC = 'gHealth'\n",
    "\n",
    "def get_next_event():\n",
    "    new_data = generate_data(n_users=1,n_meals=0,n_activities=0,gen_goals=False,gen_groups=False)\n",
    "    return new_data\n",
    "\n",
    "class Producer(threading.Thread):\n",
    "    '''Produces login events and publishes them to Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        client = KafkaClient('localhost:9092')\n",
    "        producer = SimpleProducer(client)\n",
    "        while True:\n",
    "            new_data = get_next_event()\n",
    "            for datum in new_data:\n",
    "                json_datum = json.dumps(datum).encode('utf-8')\n",
    "                producer.send_messages(KAFKA_TOPIC, json_datum)\n",
    "                time.sleep(1)\n",
    "\n",
    "#### When integrating with Spark the KafkaConsumer is replace by Spark Streaming\n",
    "# class Consumer(threading.Thread):\n",
    "#     '''Consumes users from Kafka topic.'''\n",
    "#     pass\n",
    "\n",
    "def main():\n",
    "    '''Starts producer and consumer threads.'''\n",
    "    threads = [ Producer() ]\n",
    "    for t in threads: t.start()\n",
    "    time.sleep(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s.%(name)s:%(message)s',\n",
    "        level=logging.DEBUG)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kafka_gHealth.py\n"
     ]
    }
   ],
   "source": [
    "%%file kafka_gHealth.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys,json, happybase, datetime\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "KAFKA_TOPIC = 'gHealth'\n",
    "\n",
    "def partition_data(datum):\n",
    "    datatype = datum['dataunit'].keys()[0]\n",
    "    if datatype.endswith('property'):\n",
    "        return '/'.join((datatype, datum['dataunit'][datatype]['property'].keys()[0])), datum\n",
    "    else:\n",
    "        return datatype, datum\n",
    "\n",
    "def isLocation(data):\n",
    "    return ('location' in str(data['dataunit']))\n",
    "    \n",
    "def reformat_for_hbase(data):\n",
    "    ts = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%dT%H')\n",
    "    \n",
    "    state,count = data\n",
    "    \n",
    "    rowkey = state + ':' + ts\n",
    "    value = state + ',' + str(count)\n",
    "    row = [rowkey,'d','statecount',value]\n",
    "    \n",
    "    return (rowkey,row)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sc = SparkContext(appName=\"PythonStreamingKafka_gHealth\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    zkQuorum, topic = 'localhost:2181', KAFKA_TOPIC\n",
    "    kvs = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})\n",
    "    \n",
    "    table = 'user_loc'\n",
    "    \n",
    "    conf = {\"hbase.zookeeper.quorum\": zkQuorum,\n",
    "            \"hbase.mapred.outputtable\": table,\n",
    "            \"mapreduce.outputformat.class\": \"org.apache.hadoop.hbase.mapreduce.TableOutputFormat\",\n",
    "            \"mapreduce.job.output.key.class\": \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "            \"mapreduce.job.output.value.class\": \"org.apache.hadoop.io.Writable\"}\n",
    "    keyConv = \"org.apache.spark.examples.pythonconverters.StringToImmutableBytesWritableConverter\"\n",
    "    valueConv = \"org.apache.spark.examples.pythonconverters.StringListToPutConverter\"    \n",
    "    \n",
    "    ds = kvs.window(windowDuration=100,slideDuration=10)\\\n",
    "            .map(lambda dataunit: (partition_data(json.loads(dataunit[1]))[1]))\\\n",
    "            .filter(lambda datum: isLocation(datum))\\\n",
    "            .map(lambda datum: (datum['dataunit']['user_property']['property']['location']['state'],1))\\\n",
    "            .reduceByKey(lambda cnt1,cnt2: cnt1+cnt2)\\\n",
    "            .map(lambda result: reformat_for_hbase(result))\\\n",
    "            .foreachRDD(lambda rdd: rdd.saveAsNewAPIHadoopDataset(conf=conf,\n",
    "                                                                  keyConverter=keyConv,\n",
    "                                                                  valueConverter=valueConv))\n",
    "    \n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$SPARK_HOME/bin/spark-submit \\\n",
    "    --driver-class-path spark-1.4.1-bin-hadoop2.4/lib/spark-examples-1.4.1-hadoop2.4.0.jar \\\n",
    "    --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.1 \\\n",
    "    /Users/bfetters/projects/github/DSCI6007-student/kafka_gHealth.py \\\n",
    "    localhost:2181 gHealth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting master, logging to /Users/bfetters/hbase-1.1.2/bin/../logs/hbase-bfetters-master-m4cb00kpr0.local.out\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\n",
      "starting thrift, logging to /Users/bfetters/hbase-1.1.2/bin/../logs/hbase-bfetters-thrift-m4cb00kpr0.local.out\n"
     ]
    }
   ],
   "source": [
    "!/Users/bfetters/hbase-1.1.2/bin/start-hbase.sh\n",
    "!/Users/bfetters/hbase-1.1.2/bin/hbase-daemon.sh start thrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !/Users/bfetters/hbase-1.1.2/bin/stop-hbase.sh\n",
    "# !/Users/bfetters/hbase-1.1.2/bin/hbase-daemon.sh stop thrift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import happybase\n",
    "def read_from_hbase(tablename):\n",
    "    connection = happybase.Connection(host='localhost')\n",
    "    table = connection.table(tablename)\n",
    "    for row in table.scan():\n",
    "        print row\n",
    "        \n",
    "read_from_hbase('user_count_by_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
